{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, Model, regularizers, Input\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('synthetic_thermometer_images/metadata.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "dfs1 = pd.DataFrame(data).rename(columns={'temperature':'reading'}).dropna().sample(frac=0.15, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('synthetic_thermometer_images (temp-dual)/metadata.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "dfs2 = pd.DataFrame(data).rename(columns={'temperature':'reading'}).dropna().sample(frac=0.2, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('synthetic/synthetic_thermometer_images/metadata.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "dfs3 = pd.DataFrame(data).rename(columns={'temperature':'reading'}).dropna().sample(frac=0.3, random_state=42).reset_index(drop=True)\n",
    "dfs3['image_path'] = dfs3['image_path'].apply(lambda x: 'synthetic/'+x)\n",
    "\n",
    "dfs = pd.concat([dfs1,dfs2,dfs3]).reset_index(drop=True)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('cropped_imgs.csv',dtype=str).rename(columns={'0':'image_path','1':'reading'})\n",
    "df2 = pd.read_csv('OCR2cropped_imgs.csv',dtype=str).rename(columns={'0':'image_path','1':'reading'})\n",
    "df3 = pd.read_csv('ocr3500.csv',dtype=str).rename(columns={'File Name':'image_path','Label 1':'reading'})\n",
    "df4 = pd.read_csv('ocr5000.csv',dtype=str).rename(columns={'File Name':'image_path','Label 1':'reading'})\n",
    "df5 = pd.read_csv('ocr2-2000.csv',dtype=str).rename(columns={'File Name':'image_path','Label 1':'reading'})\n",
    "\n",
    "def fun(x):\n",
    "    return ''.join(re.findall(r'[0-9.]+', str(x)))\n",
    "\n",
    "train_df = pd.concat([df1,df3,df4,df5,dfs]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df = df2.copy()\n",
    "\n",
    "\n",
    "train_df['reading'] = train_df['reading'].apply(lambda x: fun(x))\n",
    "train_df = train_df[train_df['reading'].str.len() <= 5].reset_index(drop=True)\n",
    "# train_df = train_df[train_df['reading']!=''].reset_index(drop=True)\n",
    "\n",
    "val_df['reading'] = val_df['reading'].apply(lambda x: fun(x))\n",
    "val_df = val_df[val_df['reading'].str.len() <= 5].reset_index(drop=True)\n",
    "# val_df = val_df[val_df['reading']!=''].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_texts = train_df['reading'].replace('','^').astype(str).tolist()\n",
    "val_image_texts = val_df['reading'].replace('','^').astype(str).tolist()\n",
    "\n",
    "train_image_texts[:5],val_image_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating train_image_texts and before further processing:\n",
    "chars = sorted({c for txt in train_image_texts for c in txt})   # drop stray spaces\n",
    "print(\"Characters found :\", chars)\n",
    "\n",
    "# --- Corrected character mapping ---\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}       # Shift to zero-based indices\n",
    "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "num_classes = len(chars) + 1        # Total classes: valid characters + 1 (blank token)\n",
    "\n",
    "print(\"num_classes, char_to_idx, idx_to_char:\")\n",
    "print(num_classes, char_to_idx, idx_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_len = max(len(t) for t in train_image_texts)\n",
    "\n",
    "def encode(txt):\n",
    "    seq = [char_to_idx[c] for c in txt]          # txt is *non‑empty*\n",
    "    length = len(seq)\n",
    "    seq = pad_sequences([seq], maxlen=max_label_len,\n",
    "                        padding=\"post\", value=-1) # pad with -1\n",
    "    return seq[0].astype(\"int32\"), np.int32(length)\n",
    "\n",
    "train_encoded = [encode(t) for t in train_image_texts]\n",
    "train_padded_labels, train_label_lengths = map(np.array, zip(*train_encoded))\n",
    "\n",
    "val_encoded = [encode(t) for t in val_image_texts]\n",
    "val_padded_labels, val_label_lengths = map(np.array, zip(*val_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_len,train_encoded[:5],val_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded_labels[:5], train_label_lengths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = train_df['image_path'].values.tolist()\n",
    "val_paths = val_df['image_path'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_sample(img_path, label, label_len):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [128, 256])\n",
    "    return {\n",
    "        \"image\": img,\n",
    "        \"label\": label,\n",
    "        \"label_length\": label_len            # used by the CTCLayer\n",
    "    }\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def make_dataset(paths, labels, label_lens, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels, label_lens))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(paths), reshuffle_each_iteration=True)\n",
    "    ds = (\n",
    "        ds.map(process_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .batch(batch_size)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "train_dataset = make_dataset(train_paths, train_padded_labels, train_label_lengths, shuffle=True)\n",
    "validation_dataset = make_dataset(val_paths, val_padded_labels, val_label_lengths, shuffle=False)\n",
    "\n",
    "print(\"num_classes (including CTC blank):\", num_classes)\n",
    "print(\"max_label_len :\", max_label_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "When you use a dense tensor for the targets, TensorFlow’s CTC loss (i.e. tf.keras.backend.ctc_batch_cost) uses the supplied label lengths to know which values to consider and which to ignore. In other words, if you’ve padded your labels with –1 *after* the true sequence (and you’ve correctly computed the label length), the loss function will only consider the first N valid entries and ignore the –1’s.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **Dense Target Tensor with Label Length:**\n",
    "   - Your encoded labels are a dense tensor with padding (–1) for positions beyond the true sequence.\n",
    "   - You also compute and pass `label_length`, which tells the loss function exactly how many elements in each label sequence are valid.\n",
    "   - **Result:** tf.keras.backend.ctc_batch_cost will ignore any token beyond the provided length. The –1 padding isn’t interpreted as a valid class as long as the first `label_length` entries are within 0…10.\n",
    "\n",
    "2. **CTC Blank Token vs. Pad Value:**\n",
    "   - **CTC Blank:** In your network, the blank class is at index 11, and it is used internally by CTC to allow non-overlapping outputs.\n",
    "   - **Pad Value (–1):** This value is used only to make all target sequences have the same length.\n",
    "   - **Thus:** These two serve different purposes and need not be handled together—only the valid (first `label_length`) tokens matter for the loss.\n",
    "\n",
    "3. **At Inference Time:**\n",
    "   - When decoding predictions, you might see –1’s if your decoding mechanism (or beam search) doesn’t automatically remove them. In the decoding step, you need to filter out both –1 and the blank token (11). We showed how to do that in the previous `greedy_decode` function by filtering out both.\n",
    "\n",
    "### In Summary\n",
    "\n",
    "The code you have for feeding the targets to tf.keras.backend.ctc_batch_cost does handle the –1 padding automatically—as long as the true lengths (i.e. label_length) are correctly calculated and passed. The loss will ignore any numbers beyond each sequence’s length. Just remember to filter out the –1’s during any post‑processing (e.g. when decoding the model’s predictions) so that your final output only has valid character IDs.\n",
    "\n",
    "If you decide to switch frameworks (e.g. to PyTorch’s torch.nn.CTCLoss), note that in PyTorch you must manually create a sparse representation (or otherwise mask out the –1 values) because it doesn’t support padded labels in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Assume idx_to_char is defined as:\n",
    "# idx_to_char = {0: '.', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: '^'}\n",
    "\n",
    "def process_image(path):\n",
    "    \"\"\"\n",
    "    Pre-process the image: open, correct orientation, resize, and normalize.\n",
    "    \"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = ImageOps.exif_transpose(img)  # Fix orientation\n",
    "    img = img.resize((256, 128))        # Resize as per model specification (width, height)\n",
    "    img = np.array(img).astype(np.float32)\n",
    "    img = img / 255.0                   # Normalize to [0, 1]\n",
    "    return np.expand_dims(img, axis=0)  # Shape becomes (1, 128, 256, 3)\n",
    "\n",
    "def decode_predictions(y_pred, idx_to_char, remove_placeholder=True, placeholder_token='^'):\n",
    "    \"\"\"\n",
    "    Decode CTC output using greedy decoding and optionally remove the placeholder.\n",
    "    \n",
    "    Parameters:\n",
    "        y_pred (tf.Tensor): Prediction tensor of shape (batch, time_steps, num_classes).\n",
    "        idx_to_char (dict): Mapping from indices to character strings.\n",
    "        remove_placeholder (bool): If True, remove the placeholder token from decoded text.\n",
    "        placeholder_token (str): The placeholder token to remove if found.\n",
    "    \n",
    "    Returns:\n",
    "        List of decoded text strings.\n",
    "    \"\"\"\n",
    "    # The length for each batch element (all equal to T in this case)\n",
    "    input_len = np.ones(y_pred.shape[0]) * y_pred.shape[1]\n",
    "    \n",
    "    # Use tf.keras.backend.ctc_decode for greedy decoding\n",
    "    decoded, log_probs = tf.keras.backend.ctc_decode(y_pred, input_length=input_len, greedy=True)\n",
    "    decoded_sequences = decoded[0].numpy()  # Use the first (greedy) path\n",
    "    \n",
    "    texts = []\n",
    "    for seq in decoded_sequences:\n",
    "        # Filter out the padding (-1) and map indices to characters\n",
    "        char_list = [idx_to_char.get(int(idx), \"\") for idx in seq if idx != -1]\n",
    "        text = \"\".join(char_list)\n",
    "        # Optionally remove the placeholder if it was only to denote an originally empty string\n",
    "        if remove_placeholder:\n",
    "            text = text.replace(placeholder_token, \"\")\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "def model_inference(inference_model, img_path, idx_to_char, remove_placeholder=True):\n",
    "    \"\"\"\n",
    "    Complete inference: preprocesses image, runs inference, and decodes output.\n",
    "    \"\"\"\n",
    "    # Preprocess the image\n",
    "    img = process_image(img_path)\n",
    "    \n",
    "    # Perform inference\n",
    "    y_pred = inference_model(img)\n",
    "    \n",
    "    # Decode the predictions\n",
    "    decoded_texts = decode_predictions(y_pred, idx_to_char, remove_placeholder=remove_placeholder)\n",
    "    return decoded_texts[0],img  # Return the text for the first image in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1.  Custom CTC loss layer\n",
    "# ================================================================\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=\"ctc_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = tf.keras.backend.ctc_batch_cost  # always available\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y_pred, labels, label_len = inputs        # unpack\n",
    "        batch = tf.shape(labels)[0]\n",
    "        T     = tf.shape(y_pred)[1]\n",
    "\n",
    "        # ctc_batch_cost wants shape (B, 1)\n",
    "        input_len  = tf.fill([batch, 1], T)\n",
    "        label_len  = tf.expand_dims(label_len, 1)\n",
    "\n",
    "        loss = self.loss_fn(labels, y_pred, input_len, label_len)\n",
    "        self.add_loss(loss)\n",
    "        return y_pred\n",
    "\n",
    "# ================================================================\n",
    "# 2.  CNN → Bi‑LSTM encoder‑decoder\n",
    "# ================================================================\n",
    "image_in  = Input(shape=(128, 256, 3), name=\"image\")\n",
    "label_in  = Input(shape=(max_label_len,), dtype=\"int32\", name=\"label\")\n",
    "len_in    = Input(shape=(),dtype=\"int32\", name=\"label_length\")\n",
    "\n",
    "x = image_in\n",
    "\n",
    "x = Conv2D(16, 3, use_bias=False, kernel_initializer='he_normal',\n",
    "           kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('silu')(x)\n",
    "\n",
    "x = Conv2D(16, 3, use_bias=False, kernel_initializer='he_normal',\n",
    "           kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('silu')(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(32, 3, use_bias=False, kernel_initializer='he_normal',\n",
    "           kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('silu')(x)\n",
    "\n",
    "x = Conv2D(32, 3, use_bias=False, kernel_initializer='he_normal',\n",
    "           kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('silu')(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, 3, use_bias=False, kernel_initializer='he_normal',\n",
    "           kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('silu')(x)\n",
    "\n",
    "x = Conv2D(64, 3, use_bias=False, kernel_initializer='he_normal',\n",
    "           kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('silu')(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, 3, use_bias=False, kernel_initializer='he_normal',\n",
    "           kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('silu')(x)\n",
    "\n",
    "x = Conv2D(128, 3, use_bias=False, kernel_initializer='he_normal',\n",
    "           kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('silu')(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# --- reshape: (B, H, W, C) -> (B, W, H*C) -----------------------\n",
    "x = layers.Permute((2, 1, 3))(x)                # (B, W, H, C)\n",
    "x = layers.TimeDistributed(layers.Flatten())(x) # (B, W, H*C)\n",
    "\n",
    "# --- sequence modelling -----------------------------------------\n",
    "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.1))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.1))(x)\n",
    "\n",
    "y_pred = layers.Dense(num_classes, activation=\"softmax\",name=\"softmax_dense\")(x)\n",
    "\n",
    "ctc_out = CTCLayer()([y_pred, label_in, len_in])\n",
    "\n",
    "training_model  = Model([image_in, label_in, len_in], ctc_out)\n",
    "inference_model = Model(image_in, y_pred)\n",
    "training_model.compile(optimizer=tf.keras.optimizers.Adam(5e-4))\n",
    "\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = training_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    try:\n",
    "        test_image_path =  f'ssd_cropped_images/image_{i}.png'\n",
    "\n",
    "        # Assuming inference_model is defined from your training pipeline and compiled properly\n",
    "        predicted_text,img = model_inference(inference_model, test_image_path, idx_to_char)\n",
    "        print(\"Predicted text:\", predicted_text)\n",
    "        plt.imshow(img[0])\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
