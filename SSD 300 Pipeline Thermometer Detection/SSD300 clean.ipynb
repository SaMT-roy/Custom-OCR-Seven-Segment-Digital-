{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, Model\n",
    "import time\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from math import sqrt\n",
    "\n",
    "cfg = {\n",
    "    'feature_maps':[38, 19, 10, 5, 3, 1],\n",
    "    'min_dim': 300, #\n",
    "    'steps': [8, 16, 32, 32, 100, 300], #\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 232], #\n",
    "    'max_sizes': [60, 111, 162, 213, 232, 315], #\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]], #\n",
    "    'variance': [0.1, 0.2], #\n",
    "    'clip': True, #\n",
    "}\n",
    "\n",
    "class PriorBox(tf.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.image_size = cfg['min_dim']\n",
    "        self.num_priors = len(cfg['aspect_ratios'])\n",
    "        self.variance = cfg['variance'] or [0.1]\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.aspect_ratios = cfg['aspect_ratios']\n",
    "        self.clip = cfg['clip']\n",
    "        for v in self.variance:\n",
    "            if v <= 0:\n",
    "                raise ValueError('Variances must be greater than 0')\n",
    "\n",
    "    def __call__(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.image_size / self.steps[k]\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                s_k = self.min_sizes[k] / self.image_size\n",
    "                mean.append([cx, cy, s_k, s_k])\n",
    "\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k] / self.image_size))\n",
    "                mean.append([cx, cy, s_k_prime, s_k_prime])\n",
    "\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean.append([cx, cy, s_k * sqrt(ar), s_k / sqrt(ar)])\n",
    "                    mean.append([cx, cy, s_k / sqrt(ar), s_k * sqrt(ar)])\n",
    "\n",
    "        output = tf.constant(mean, dtype=tf.float32)\n",
    "        if self.clip:\n",
    "            output = tf.clip_by_value(output, clip_value_min=0, clip_value_max=1)\n",
    "        return output\n",
    "    \n",
    "def convert_scale(matrix,scale,hImage,wImage):\n",
    "    if scale == 'abs':\n",
    "        return tf.stack([matrix[:,0]*wImage,\n",
    "        matrix[:,1]*hImage,\n",
    "        matrix[:,2]*wImage,\n",
    "        matrix[:,3]*hImage],axis=-1)\n",
    "\n",
    "    elif scale == 'rel':\n",
    "        return tf.stack([matrix[:,0]/wImage,\n",
    "        matrix[:,1]/hImage,\n",
    "        matrix[:,2]/wImage,\n",
    "        matrix[:,3]/hImage],axis=-1)    \n",
    "\n",
    "    \n",
    "def convert_format(out,format):\n",
    "    if format == 'x1y1x2y2':\n",
    "        return tf.stack([out[...,0]-out[...,2]/2.0,\n",
    "        out[...,1]-out[...,3]/2.0,\n",
    "        out[...,0]+out[...,2]/2.0,\n",
    "        out[...,1]+out[...,3]/2.0]\n",
    "        ,axis=-1)\n",
    "\n",
    "    elif format == 'xywh':\n",
    "        return tf.stack([(out[...,0]+out[...,2])/2.0,\n",
    "        (out[...,1]+out[...,3])/2.0,\n",
    "        out[...,2]-out[...,0],\n",
    "        out[...,3]-out[...,1],\n",
    "        out[...,4]],axis=-1)\n",
    "    \n",
    "\n",
    "\n",
    "def get_priors_featureboxes(cfg):\n",
    "    \n",
    "    priorbox = PriorBox(cfg)\n",
    "    priors = priorbox()\n",
    "\n",
    "    # Our generated prior boxes are from 0 to 1 in range. We need to multiply with the scale of our image i.e 300\n",
    "\n",
    "    feature_box = convert_scale(priors,'abs',300,300)\n",
    "    feature_box_conv = convert_format(feature_box,'x1y1x2y2')\n",
    "\n",
    "    feature_box_conv = tf.clip_by_value(feature_box_conv, clip_value_min=0, clip_value_max=300)\n",
    "\n",
    "    return priors,feature_box,feature_box_conv\n",
    "\n",
    "\n",
    "# priors,feature_box,feature_box_conv = get_priors_featureboxes(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two sets of boxes.\n",
    "    The IoU is a measure of the overlap between two bounding boxes.\n",
    "\n",
    "    Args:\n",
    "    box1 -- tensor of shape (N, 4), ground truth bounding boxes\n",
    "    box2 -- tensor of shape (M, 4), proposed anchor boxes\n",
    "\n",
    "    Returns:\n",
    "    iou -- tensor of shape (N, M), IoU values\n",
    "    \"\"\"\n",
    "    # Convert boxes to float32 for precision during division\n",
    "    box1 = tf.cast(box1, dtype=tf.float32)\n",
    "    box2 = tf.cast(box2, dtype=tf.float32)\n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    x1 = tf.math.maximum(box1[:, None, 0], box2[:, 0])\n",
    "    y1 = tf.math.maximum(box1[:, None, 1], box2[:, 1])\n",
    "    x2 = tf.math.minimum(box1[:, None, 2], box2[:, 2])\n",
    "    y2 = tf.math.minimum(box1[:, None, 3], box2[:, 3])\n",
    "\n",
    "    # Compute area of intersection rectangle\n",
    "    intersectionArea = tf.math.maximum(0.0, x2 - x1) * tf.math.maximum(0.0, y2 - y1)\n",
    "\n",
    "    # Compute area of both bounding boxes\n",
    "    box1Area = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    box2Area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "\n",
    "    # Compute union area by using inclusion-exclusion principle\n",
    "    unionArea = tf.math.maximum(1e-10, box1Area[:, None] + box2Area - intersectionArea)\n",
    "\n",
    "    # Compute IoU by dividing intersection area by union area\n",
    "    iou = intersectionArea / unionArea\n",
    "\n",
    "    # Clip the values to be between 0 and 1 as IoU cannot exceed 1\n",
    "    return tf.clip_by_value(iou, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def df_match(labels, iou_matrix):\n",
    "    \"\"\"\n",
    "    Match each proposed anchor box with the ground truth box that has the highest IoU.\n",
    "\n",
    "    Args:\n",
    "    labels -- tensor of shape (K, 4), ground truth labels\n",
    "    iou_matrix -- tensor of shape (N, K), IoU values for each anchor against each ground truth\n",
    "\n",
    "    Returns:\n",
    "    gt_box -- tensor of shape (N, 4), matched ground truth boxes for each anchor\n",
    "    matched -- tensor of shape (N,), indicator whether matching IoU is above threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the maximum IoU value for each anchor box and its corresponding ground truth box index\n",
    "    max_values = tf.reduce_max(iou_matrix,axis=1)\n",
    "    max_idx = tf.math.argmax(iou_matrix,axis=1) \n",
    "\n",
    "    # Determine if the maximum IoU value for each anchor is above the threshold (0.5 in this case)\n",
    "    matched = tf.cast(tf.math.greater_equal(max_values,0.5),dtype=tf.float32) \n",
    "\n",
    "    # Gather the matched ground truth boxes based on the indices from max_idx\n",
    "    gt_box = tf.gather(labels,max_idx) \n",
    "\n",
    "    return gt_box, matched\n",
    "\n",
    "\n",
    "def normalised_ground_truth(matched_boxes, feature_box, return_format):\n",
    "    \n",
    "    \"\"\"Normalizes ground truth boxes based on the anchor box dimensions.\n",
    "\n",
    "    This function encodes or decodes ground truth boxes relative to the provided \n",
    "    anchor boxes. The encoding and decoding processes are essential for training\n",
    "    the SSD model.\n",
    "\n",
    "    Args:\n",
    "    matched_boxes: A tensor of shape [N, 4] representing the ground truth boxes \n",
    "      that have been matched to the anchor boxes.\n",
    "    feature_box: A tensor of shape [1, 4] representing the anchor box \n",
    "      dimensions.\n",
    "    return_format: A string indicating the desired output format. \n",
    "      \"encode\": Returns encoded boxes.\n",
    "      \"decode\": Returns decoded boxes.\n",
    "\n",
    "    Returns:\n",
    "    A tensor of shape [N, 4] representing the encoded or decoded ground truth \n",
    "    boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data types to float32\n",
    "    matched_boxes = tf.cast(matched_boxes,tf.float32)\n",
    "    feature_box = tf.cast(feature_box,tf.float32)\n",
    "\n",
    "    if return_format == \"encode\":\n",
    "        \n",
    "        \"\"\"\n",
    "        Encoding ground truth boxes:\n",
    "\n",
    "        This process transforms the ground truth boxes into relative offsets and \n",
    "        scale factors with respect to the anchor box. The offsets are normalized \n",
    "        by dividing them by the anchor box's width and height. The scale factors are \n",
    "        represented as the logarithm of the ratio between the ground truth box's \n",
    "        width/height and the anchor box's width/height.\n",
    "\n",
    "        The normalized values are then further scaled by constants (0.1 for \n",
    "        offsets and 0.2 for scale factors) to ensure stability during training.\n",
    "        \"\"\"\n",
    "        x_offset = (matched_boxes[:, 0] - feature_box[:, 0]) / feature_box[:, 2]\n",
    "        y_offset = (matched_boxes[:, 1] - feature_box[:, 1]) / feature_box[:, 3]\n",
    "        w_scale = tf.math.log(matched_boxes[:, 2] / feature_box[:, 2])\n",
    "        h_scale = tf.math.log(matched_boxes[:, 3] / feature_box[:, 3])\n",
    "\n",
    "        encoded_boxes = tf.stack([x_offset, y_offset, w_scale, h_scale], axis=-1)\n",
    "        return encoded_boxes / [0.1, 0.1, 0.2, 0.2]\n",
    "\n",
    "    elif return_format == \"decode\":\n",
    "        \n",
    "        \"\"\"\n",
    "        Decoding ground truth boxes:\n",
    "\n",
    "        This process reverses the encoding process to obtain the original ground\n",
    "        truth boxes from the encoded values. It involves:\n",
    "        - Reversing the scaling applied during encoding.\n",
    "        - Using the encoded values to calculate the center coordinates (x_center,\n",
    "        y_center) and width (w) and height (h) of the boxes relative to the anchor \n",
    "        box.\n",
    "        - Applying the exponential function to the scale factors to recover the \n",
    "        original width and height ratios.\n",
    "        - Finally, multiplying the calculated dimensions by the anchor box's width \n",
    "        and height to obtain the absolute dimensions of the ground truth box.\n",
    "        \"\"\"\n",
    "        encoded_boxes = matched_boxes * [0.1, 0.1, 0.2, 0.2]\n",
    "        x_center = encoded_boxes[:, 0] * feature_box[:, 2] + feature_box[:, 0]\n",
    "        y_center = encoded_boxes[:, 1] * feature_box[:, 3] + feature_box[:, 1]\n",
    "        w = tf.math.exp(encoded_boxes[:, 2]) * feature_box[:, 2]\n",
    "        h = tf.math.exp(encoded_boxes[:, 3]) * feature_box[:, 3]\n",
    "\n",
    "        decoded_boxes = tf.stack([x_center, y_center, w, h], axis=-1)\n",
    "        return decoded_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14256,\n",
       " {'object': [{'name': 'objects',\n",
       "    'xmin': 104,\n",
       "    'xmax': 154,\n",
       "    'ymin': 218,\n",
       "    'ymax': 237}],\n",
       "  'filename': 'JPEGImages/1001_jpg.rf.dfc0798279d416e17e5b66b7732b0d02.jpg',\n",
       "  'width': 300,\n",
       "  'height': 300})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "# File path to save and read the data\n",
    "file_path = \"train_image_annotations.json\"\n",
    "\n",
    "# Read the data back from the file\n",
    "with open(file_path, \"r\") as f:\n",
    "    train_image = json.load(f)\n",
    "\n",
    "len(train_image),train_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ----- Your helper functions -----\n",
    "def fun(x, H, W):\n",
    "    w = x['width']\n",
    "    h = x['height']\n",
    "    \n",
    "    attr = []\n",
    "    for i in x['object']:\n",
    "        t = {}\n",
    "        xmin = i['xmin'] * W / w\n",
    "        xmax = i['xmax'] * W / w\n",
    "        ymin = i['ymin'] * H / h\n",
    "        ymax = i['ymax'] * H / h\n",
    "        name = i['name']\n",
    "        t['name'] = name\n",
    "        t['xmin'] = xmin\n",
    "        t['ymin'] = ymin\n",
    "        t['xmax'] = xmax\n",
    "        t['ymax'] = ymax\n",
    "        attr.append(t)\n",
    "    \n",
    "    image = cv2.imread(x['filename'])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (W, H))\n",
    "    image = image / 255.0\n",
    "    return image, attr\n",
    "\n",
    "def fun_binary(x):  \n",
    "    # Note: update this logic as needed because here x is a string.\n",
    "    # This dummy example converts non-'0' labels to 1 and '0' to 0.\n",
    "    return 0 if x != '0' else 1\n",
    "\n",
    "# Assuming iou, df_match, convert_format, and normalised_ground_truth are defined elsewhere.\n",
    "def main(label, feature_box, feature_box_conv):\n",
    "    iou_matrix = iou(feature_box_conv, label)\n",
    "    gt_box, matched = df_match(convert_format(label, 'xywh'), iou_matrix)\n",
    "    boxes = gt_box[:, :4]\n",
    "    classes = gt_box[:, 4]\n",
    "    \n",
    "    classes = tf.cast(classes, dtype=tf.int32)\n",
    "    matched = tf.cast(matched, dtype=tf.int32)\n",
    "    classes = tf.cast(classes * matched, dtype=tf.int32)\n",
    "    classes = tf.one_hot(classes, depth=1, dtype=tf.float32)\n",
    "    normalised_gtbox = normalised_ground_truth(boxes, feature_box, 'encode')\n",
    "    df_box = tf.concat((normalised_gtbox, classes), axis=-1)\n",
    "    df_box.set_shape([feature_box.shape[0], 4 + 1])\n",
    "    return df_box\n",
    "\n",
    "# ----- Define a generator function -----\n",
    "def data_generator(train_image, feature_box, feature_box_conv):\n",
    "    for annotation in train_image:\n",
    "        # Process image and extract object attributes.\n",
    "        image, attr = fun(annotation, 300, 300)  # returns image (300x300x3) and a list of object dicts\n",
    "        \n",
    "        # Build coordinate array from the attributes.\n",
    "        coords = []\n",
    "        for a in attr:\n",
    "            xmin, ymin, xmax, ymax, name = a['xmin'], a['ymin'], a['xmax'], a['ymax'], a['name']\n",
    "            # Convert the label to a binary value.\n",
    "            # (Adjust the fun_binary logic as needed for your labeling.)\n",
    "            label = 1 - fun_binary(name)\n",
    "            coords.append([xmin, ymin, xmax, ymax, label])\n",
    "        \n",
    "        coords = np.array(coords, dtype=np.float32)\n",
    "        \n",
    "        # Compute the final coordinates using your main function.\n",
    "        final_coord = main(coords, feature_box, feature_box_conv)\n",
    "        # If main returns a tf.Tensor, convert it to a NumPy array.\n",
    "        if isinstance(final_coord, tf.Tensor):\n",
    "            final_coord = final_coord.numpy()\n",
    "            \n",
    "        # Yield the image and corresponding target coordinates.\n",
    "        yield image.astype(np.float32), final_coord.astype(np.float32)\n",
    "\n",
    "# ----- Prepare the tf.data.Dataset -----\n",
    "# Assume that `train_image` is your list of annotation dictionaries.\n",
    "# Assume get_priors_featureboxes(cfg) returns feature_box and feature_box_conv.\n",
    "priors, feature_box, feature_box_conv = get_priors_featureboxes(cfg)\n",
    "num_priors = feature_box.shape[0]  # final_coord shape will be (num_priors, 5)\n",
    "\n",
    "# Define the output signature for the Dataset.\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(300, 300, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(num_priors, 5), dtype=tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split your annotation list (train_image) into train and validation sets.\n",
    "train_imgs, val_imgs = train_test_split(train_image, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_imgs, feature_box, feature_box_conv),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(300, 300, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(feature_box.shape[0], 5), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create validation dataset\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_imgs, feature_box, feature_box_conv),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(300, 300, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(feature_box.shape[0], 5), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8732</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>), │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,370,902</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8732</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]  │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8732</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8732\u001b[0m, \u001b[38;5;34m4\u001b[0m), │  \u001b[38;5;34m3,370,902\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8732\u001b[0m, \u001b[38;5;34m1\u001b[0m)]  │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8732\u001b[0m, \u001b[38;5;34m5\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,370,902</span> (12.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,370,902\u001b[0m (12.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,370,902</span> (12.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,370,902\u001b[0m (12.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def total_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    # Create a mask for positive samples\n",
    "    pos_mask = tf.cast(tf.equal(tf.squeeze(y_true[:, :, 4:5], axis=-1), 0.0), tf.float32)\n",
    "    num_pos = tf.maximum(1.0, tf.cast(tf.math.count_nonzero(pos_mask, axis=-1), tf.float32))\n",
    "\n",
    "    # Calculate localization loss\n",
    "    loc_loss = tf.compat.v1.losses.huber_loss(labels=y_true[:, :, :4],\n",
    "                                              predictions=y_pred[:, :, :4],\n",
    "                                              reduction=\"none\")\n",
    "    \n",
    "    loc_loss = tf.reduce_sum(loc_loss, axis=-1)\n",
    "    loc_loss = tf.where(tf.equal(pos_mask, 1.0), loc_loss, 0.0)\n",
    "    loc_loss = tf.reduce_sum(loc_loss, axis=-1)\n",
    "    loc_loss = loc_loss / num_pos\n",
    "\n",
    "    # Calculate classification loss\n",
    "    cce = tf.losses.BinaryCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    cross_entropy = cce(y_true[:, :, 4:], y_pred[:, :, 4:])\n",
    "\n",
    "    # Set negative to positive ratio\n",
    "    num_neg =  num_pos\n",
    "\n",
    "    # Perform hard negative mining\n",
    "    neg_cross_entropy = tf.where(tf.equal(pos_mask, 0.0), cross_entropy, 0.0)\n",
    "    sorted_dfidx = tf.cast(tf.argsort(neg_cross_entropy, direction='DESCENDING', axis=-1), tf.int32)\n",
    "    rank = tf.cast(tf.argsort(sorted_dfidx, axis=-1), tf.int32)\n",
    "    num_neg = tf.cast(num_neg, dtype=tf.int32)\n",
    "    neg_loss = tf.where(rank < tf.expand_dims(num_neg, axis=1), neg_cross_entropy, 0.0)\n",
    "\n",
    "    # Calculate positive loss\n",
    "    pos_loss = tf.where(tf.equal(pos_mask, 1.0), cross_entropy, 0.0)\n",
    "    clas_loss = tf.reduce_sum(pos_loss + neg_loss, axis=-1)\n",
    "    clas_loss = clas_loss / num_pos\n",
    "\n",
    "    # Combine losses\n",
    "    total_loss = loc_loss + clas_loss\n",
    "    return total_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, MaxPool2D, Reshape, Concatenate, MaxPooling2D, ZeroPadding2D, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "l2_reg = l2_regularization = 0.0005\n",
    "\n",
    "class L2Normalization(Layer):\n",
    "    \"\"\"Normalizing different scale features for fusion.\n",
    "    paper: https://arxiv.org/abs/1506.04579\n",
    "    inputs:\n",
    "        feature_map = (batch_size, feature_map_height, feature_map_width, depth)\n",
    "    outputs:\n",
    "        normalized_feature_map = (batch_size, feature_map_height, feature_map_width, depth)\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_factor, **kwargs):\n",
    "        super(L2Normalization, self).__init__(**kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(L2Normalization, self).get_config()\n",
    "        config.update({\"scale_factor\": self.scale_factor})\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Network need to learn scale factor for each channel\n",
    "        init_scale_factor = tf.fill((input_shape[-1],), float(self.scale_factor))\n",
    "        self.scale = tf.Variable(init_scale_factor, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.l2_normalize(inputs, axis=-1) * self.scale\n",
    "\n",
    "def buildSSD_VGG16(n_classes, n_boxes = [4, 6, 6, 6, 4, 4]):\n",
    "\n",
    "    x = Input(shape=(300, 300, 3))\n",
    "\n",
    "    conv1_1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_1')(x)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool1')(conv1_1)\n",
    "\n",
    "    conv2_1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_1')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool2')(conv2_1)\n",
    "\n",
    "    conv3_1 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_1')(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3')(conv3_1)\n",
    "\n",
    "    conv4_1 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_1')(pool3)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool4')(conv4_1)\n",
    "\n",
    "    conv5_1 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_1')(pool4)\n",
    "    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='pool5')(conv5_1)\n",
    "\n",
    "    fc6 = Conv2D(512, (3, 3), dilation_rate=(6, 6), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc6')(pool5)\n",
    "\n",
    "    fc7 = Conv2D(512, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7')(fc6)\n",
    "\n",
    "    conv6_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_1')(fc7)\n",
    "    conv6_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv6_padding')(conv6_1)\n",
    "    conv6_2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2')(conv6_1)\n",
    "\n",
    "    conv7_1 = Conv2D(64, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_1')(conv6_2)\n",
    "    conv7_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv7_padding')(conv7_1)\n",
    "    conv7_2 = Conv2D(128, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2')(conv7_1)\n",
    "\n",
    "    conv8_1 = Conv2D(64, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_1')(conv7_2)\n",
    "    conv8_2 = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2')(conv8_1)\n",
    "\n",
    "    conv9_1 = Conv2D(64, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_1')(conv8_2)\n",
    "    conv9_2 = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2')(conv9_1)\n",
    "\n",
    "    # Feed conv4_3 into the L2 normalization layer\n",
    "    conv4_3_norm = L2Normalization(scale_factor=20, name='conv4_3_norm')(conv4_1)\n",
    "\n",
    "    ### Build the convolutional predictor layers on top of the base network\n",
    "\n",
    "    # We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`\n",
    "    # Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`\n",
    "    conv4_3_norm_mbox_conf = Conv2D(n_boxes[0] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_conf')(conv4_3_norm)\n",
    "    fc7_mbox_conf = Conv2D(n_boxes[1] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_conf')(fc7)\n",
    "    conv6_2_mbox_conf = Conv2D(n_boxes[2] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_conf')(conv6_2)\n",
    "    conv7_2_mbox_conf = Conv2D(n_boxes[3] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_conf')(conv7_2)\n",
    "    conv8_2_mbox_conf = Conv2D(n_boxes[4] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_conf')(conv8_2)\n",
    "    conv9_2_mbox_conf = Conv2D(n_boxes[5] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_conf')(conv9_2)\n",
    "    # We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`\n",
    "    # Output shape of the localization layers: `(batch, height, width, n_boxes * 4)`\n",
    "    conv4_3_norm_mbox_loc = Conv2D(n_boxes[0] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_loc')(conv4_3_norm)\n",
    "    fc7_mbox_loc = Conv2D(n_boxes[1] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_loc')(fc7)\n",
    "    conv6_2_mbox_loc = Conv2D(n_boxes[2] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_loc')(conv6_2)\n",
    "    conv7_2_mbox_loc = Conv2D(n_boxes[3] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_loc')(conv7_2)\n",
    "    conv8_2_mbox_loc = Conv2D(n_boxes[4] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_loc')(conv8_2)\n",
    "    conv9_2_mbox_loc = Conv2D(n_boxes[5] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_loc')(conv9_2)\n",
    "\n",
    "    conv4_3_norm_mbox_conf_reshape = Reshape((-1, n_classes), name='conv4_3_norm_mbox_conf_reshape')(conv4_3_norm_mbox_conf)\n",
    "    fc7_mbox_conf_reshape = Reshape((-1, n_classes), name='fc7_mbox_conf_reshape')(fc7_mbox_conf)\n",
    "    conv6_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv6_2_mbox_conf_reshape')(conv6_2_mbox_conf)\n",
    "    conv7_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv7_2_mbox_conf_reshape')(conv7_2_mbox_conf)\n",
    "    conv8_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv8_2_mbox_conf_reshape')(conv8_2_mbox_conf)\n",
    "    conv9_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv9_2_mbox_conf_reshape')(conv9_2_mbox_conf)\n",
    "\n",
    "    conv4_3_norm_mbox_loc_reshape = Reshape((-1, 4), name='conv4_3_norm_mbox_loc_reshape')(conv4_3_norm_mbox_loc)\n",
    "    fc7_mbox_loc_reshape = Reshape((-1, 4), name='fc7_mbox_loc_reshape')(fc7_mbox_loc)\n",
    "    conv6_2_mbox_loc_reshape = Reshape((-1, 4), name='conv6_2_mbox_loc_reshape')(conv6_2_mbox_loc)\n",
    "    conv7_2_mbox_loc_reshape = Reshape((-1, 4), name='conv7_2_mbox_loc_reshape')(conv7_2_mbox_loc)\n",
    "    conv8_2_mbox_loc_reshape = Reshape((-1, 4), name='conv8_2_mbox_loc_reshape')(conv8_2_mbox_loc)\n",
    "    conv9_2_mbox_loc_reshape = Reshape((-1, 4), name='conv9_2_mbox_loc_reshape')(conv9_2_mbox_loc)\n",
    "\n",
    "    mbox_conf = Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_mbox_conf_reshape,\n",
    "                                                        fc7_mbox_conf_reshape,\n",
    "                                                        conv6_2_mbox_conf_reshape,\n",
    "                                                        conv7_2_mbox_conf_reshape,\n",
    "                                                        conv8_2_mbox_conf_reshape,\n",
    "                                                        conv9_2_mbox_conf_reshape])\n",
    "\n",
    "    mbox_loc = Concatenate(axis=1, name='mbox_loc')([conv4_3_norm_mbox_loc_reshape,\n",
    "                                                        fc7_mbox_loc_reshape,\n",
    "                                                        conv6_2_mbox_loc_reshape,\n",
    "                                                        conv7_2_mbox_loc_reshape,\n",
    "                                                        conv8_2_mbox_loc_reshape,\n",
    "                                                        conv9_2_mbox_loc_reshape])\n",
    "\n",
    "    mbox_conf_sigmoid = Activation('sigmoid', name='mbox_conf_sigmoid')(mbox_conf)\n",
    "    ssd_VGG16 = Model(inputs=x, outputs=[mbox_loc, mbox_conf_sigmoid])\n",
    "\n",
    "    return ssd_VGG16\n",
    "\n",
    "def SSD():\n",
    "    \n",
    "    inputs = Input((300,300,3))\n",
    "    \n",
    "    model = buildSSD_VGG16(n_classes=1)\n",
    "    \n",
    "    loc,clas = model(inputs)\n",
    "    \n",
    "    pred = Concatenate(axis=-1)([loc, clas])\n",
    "    \n",
    "    return Model(inputs=[inputs], outputs=[pred])\n",
    "\n",
    "model = SSD()\n",
    "\n",
    "model.compile(optimizer='adam',loss=total_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saptarshimallikthakur/Desktop/CLIP/lib/python3.12/site-packages/keras/src/models/functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(None, 300, 300, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    401/Unknown \u001b[1m401s\u001b[0m 986ms/step - loss: 5.1533"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saptarshimallikthakur/Desktop/CLIP/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 1s/step - loss: 5.1505 - val_loss: 2.8594 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 1s/step - loss: 2.6946 - val_loss: 2.3654 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 1s/step - loss: 2.3086 - val_loss: 2.2242 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 1s/step - loss: 2.1632 - val_loss: 2.1301 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 1s/step - loss: 2.0775 - val_loss: 2.1065 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 1s/step - loss: 2.0185 - val_loss: 2.0796 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 1s/step - loss: 1.9767 - val_loss: 2.0533 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - loss: 1.9447 - val_loss: 2.0167 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - loss: 1.9250 - val_loss: 2.0102 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - loss: 1.9068 - val_loss: 1.9968 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - loss: 1.8889 - val_loss: 2.0226 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962ms/step - loss: 1.8768\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.8768 - val_loss: 2.0064 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 1s/step - loss: 1.8492 - val_loss: 1.9845 - learning_rate: 8.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.8209 - val_loss: 1.9487 - learning_rate: 8.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - loss: 1.7957 - val_loss: 1.9331 - learning_rate: 8.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 1s/step - loss: 1.7779 - val_loss: 1.9283 - learning_rate: 8.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 1s/step - loss: 1.7695 - val_loss: 1.9417 - learning_rate: 8.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977ms/step - loss: 1.7670\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 1s/step - loss: 1.7670 - val_loss: 1.9531 - learning_rate: 8.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.7715 - val_loss: 1.9826 - learning_rate: 6.4000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961ms/step - loss: 1.7591\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.7591 - val_loss: 1.9595 - learning_rate: 6.4000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.7411 - val_loss: 1.8890 - learning_rate: 5.1200e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.7145 - val_loss: 1.8743 - learning_rate: 5.1200e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 1s/step - loss: 1.6907 - val_loss: 1.8697 - learning_rate: 5.1200e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 1s/step - loss: 1.6729 - val_loss: 1.8719 - learning_rate: 5.1200e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964ms/step - loss: 1.6608\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - loss: 1.6608 - val_loss: 1.8728 - learning_rate: 5.1200e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - loss: 1.6571 - val_loss: 1.8672 - learning_rate: 4.0960e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.6546 - val_loss: 1.8648 - learning_rate: 4.0960e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.6477 - val_loss: 1.8560 - learning_rate: 4.0960e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - loss: 1.6420 - val_loss: 1.8550 - learning_rate: 4.0960e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - loss: 1.6336 - val_loss: 1.8543 - learning_rate: 4.0960e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - loss: 1.6271 - val_loss: 1.8546 - learning_rate: 4.0960e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - loss: 1.6190 - val_loss: 1.8525 - learning_rate: 4.0960e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 1s/step - loss: 1.6090 - val_loss: 1.8556 - learning_rate: 4.0960e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957ms/step - loss: 1.6014\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 1s/step - loss: 1.6014 - val_loss: 1.8528 - learning_rate: 4.0960e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - loss: 1.6061 - val_loss: 1.8802 - learning_rate: 3.2768e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960ms/step - loss: 1.6162\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - loss: 1.6162 - val_loss: 1.8838 - learning_rate: 3.2768e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - loss: 1.6087 - val_loss: 1.8699 - learning_rate: 2.6214e-04\n",
      "Epoch 37: early stopping\n",
      "Restoring model weights from the end of the best epoch: 32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30b63b680>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Define your callbacks (as before) -----\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.8,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model using both training and validation datasets.\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('SSD300.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save numpy array\n",
    "np.save('priors.npy', priors)\n",
    "# Load numpy array\n",
    "loaded_priors = np.load('priors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to ONNX and saved as SSD300 fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "\n",
    "# Define the input signature based on your model's input shape.\n",
    "spec = (tf.TensorSpec(model.inputs[0].shape, tf.float32, name=\"input\"),)\n",
    "\n",
    "# Convert the model to ONNX format and save it.\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, output_path=\"SSD300 fp32.onnx\")\n",
    "print(\"Model converted to ONNX and saved as SSD300 fp32.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Static INT8 quantization complete: SSD300_int8.onnx\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from onnxruntime.quantization import (\n",
    "    quantize_static,\n",
    "    CalibrationDataReader,\n",
    "    QuantFormat,\n",
    "    QuantType,\n",
    ")\n",
    "\n",
    "\n",
    "class FileCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, image_paths, input_name=\"input\", preprocess_fn=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.input_name = input_name\n",
    "        self.preprocess = preprocess_fn or self._default_preprocess\n",
    "        self.index = 0\n",
    "\n",
    "    def _default_preprocess(self, path):\n",
    "        import cv2, numpy as np\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (300, 300))\n",
    "        return img.astype(np.float32) / 255.0\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.index >= len(self.image_paths):\n",
    "            return None\n",
    "\n",
    "        # 1) load & preprocess a single image: shape (H, W, C)\n",
    "        img = self.preprocess(self.image_paths[self.index])\n",
    "        self.index += 1\n",
    "\n",
    "        # 2) add batch dimension → shape (1, H, W, C)\n",
    "        batched = img[np.newaxis, ...]\n",
    "\n",
    "        # 3) return dict with correct rank\n",
    "        return {self.input_name: batched}\n",
    "\n",
    "\n",
    "# 1. Point at all the images in your folder:\n",
    "calib_folder = \"JPEGImages_rotated\"\n",
    "# match .jpg, .jpeg, .png (adjust extensions as needed)\n",
    "image_paths = glob.glob(f\"{calib_folder}/*.[jp][pn]g\")\n",
    "\n",
    "# 2. Create the reader (streams one file at a time)\n",
    "reader = FileCalibrationDataReader(image_paths, input_name=\"input\")\n",
    "\n",
    "# 3. Run static INT8 quantization\n",
    "quantize_static(\n",
    "    model_input=\"SSD300 fp32.onnx\",\n",
    "    model_output=\"SSD300 int8.onnx\",\n",
    "    calibration_data_reader=reader,\n",
    "    quant_format=QuantFormat.QDQ,    # or QuantFormat.QOperator\n",
    "    activation_type=QuantType.QInt8,\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "\n",
    "print(\"✅ Static INT8 quantization complete: SSD300_int8.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
